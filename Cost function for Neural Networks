

library(R.matlab)

###Creating the Feed forward Network Assuming Theta1 and Theta2 have been trained.


data <- readMat("..../mlclass-ex4/ex4data1.mat")
weights_initial <- readMat("...../mlclass-ex4/ex4weights.mat")
weights_initial_Theta1 <- data.frame(weights_initial$Theta1)
weights_initial_Theta2 <- data.frame(weights_initial$Theta2)


data_x <- data.frame(data$X)

###Adding bias unit in the first layer
data_x <- data.frame(cbind(as.vector(rep(1, nrow(data_x))),data_x) )
colnames(data_x)[1] <- c("bias")

data_y <- data.frame(data$y)

nrow(data_x)

sigmoid_function <- function(z)
{
  return(1/(1+exp(-z)))
  
}


###Transformed Xs using Thetas  Weights * T in the second layer
z2 <- as.matrix(data_x) %*% t(as.matrix(weights_initial_Theta1))
##Activation in the second layer
a2 <- sigmoid_function(z2)

##Adding a bias unit to all the training records
z2  <- as.matrix(cbind(as.vector(rep(1, nrow(a2))),a2))
###Transformed Xs using Thetas  Weights * T in the second layer
z3 <- as.matrix(z2) %*% t(as.matrix(weights_initial_Theta2))

###Activation in the third layer
a3 <- sigmoid_function(z3)

y_mat <- matrix(0, nrow = nrow(data_x), ncol = 10)

for( i in 1:nrow(data_x))
{
  a <- data_y[i,1]
  y_mat[i,a] <- 1
  
}

### Non-regularized Cost function for Neural Networks###
cost_function_nn <- function(a3,y_mat)
{
  
  -1/(nrow(a3)) *sum(y_mat * log(a3) + (1-y_mat)* log(1-a3))

}
##Cost = 0.2876292 for unregularized cost function as per the  exercise document

weights_initial_Theta1_reg <- weights_initial_Theta1[2:ncol(weights_initial_Theta1)]

weights_initial_Theta2_reg <- weights_initial_Theta2[2:ncol(weights_initial_Theta2)]

cost_function_nn_regularized <- function(a3,y_mat,lambda)
{
  
  -1/(nrow(a3)) *sum(y_mat * log(a3) + (1-y_mat)* log(1-a3)) + lambda/(2*nrow(a3))* (sum((weights_initial_Theta1_reg)^2) + sum((weights_initial_Theta2_reg)^2))
  
  
}
##For lambda = 1 Cost  is 0.3837699 as per the  exercise document
